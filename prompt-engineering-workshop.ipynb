{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b9d191f-5d63-499d-8095-1795cadab9e8",
   "metadata": {},
   "source": [
    "# Prompt Engineering Quest to 300 Workshop\n",
    "Welcome to the Prompt Engineering Quest to 300 Workshop. This workshop is designed as 'Quest to 300' where you will be learning prompt best practices by solving quest questions.  \n",
    "The workshop starts by introducing various prompting concepts which should be used later to solve the quest questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1dc407",
   "metadata": {},
   "source": [
    "## Workshop model providers\n",
    "This workshop will use two model providers Anthropic and Mistral AI available on Amazon Bedrock.\n",
    "\n",
    "__Anthropic__\n",
    "Anthropic's Claude family of models – Haiku, Sonnet, and Opus – allow customers to choose the exact combination of intelligence, speed, and cost that suits their business needs. Claude 3 Opus, the company's most capable model, has set a market standard on benchmarks. All of the latest Claude models have vision capabilities that enable them to process and analyze image data, meeting a growing demand for multimodal AI systems that can handle diverse data formats. While the family offers impressive performance across the board, Claude 3 Haiku is one of the most affordable and fastest options on the market for its intelligence category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69f0e74",
   "metadata": {},
   "source": [
    "\n",
    "__Mistral AI__\n",
    "Mistral AI is a small creative team with high scientific standards. Mistral models are efficient, helpful and trustworthy through ground-breaking innovations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc746d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip --quiet\n",
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c0dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Default C3 model id: anthropic.claude-3-haiku-20240307-v1:0\n",
    "# DEfault Mistral model id: mistral.mistral-7b-instruct-v0:2\n",
    "\n",
    "from completions import get_c3_completion, get_mistral_completion\n",
    "\n",
    "PROMPT = \"Hello, Claude!\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": PROMPT}\n",
    "]\n",
    "print(f'\\nClaude:\\n{get_c3_completion(messages)}')\n",
    "\n",
    "PROMPT = \"<s>[INST] Hello, Mistral [/INST]\"\n",
    "print(f'\\nMistral:\\n{get_mistral_completion(PROMPT)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca903ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip --quiet\n",
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7789fc8f-03c5-4b22-a527-f63354249e39",
   "metadata": {},
   "source": [
    "## Prompt structures\n",
    "Most LLM prompts contain one or more conceptual prompt components that can help direct the model attention to the correct direction. These components help articulate the prompt designer intent. Prompts that lack certain components can result in long, inefficient conversations, with many query refinements.\n",
    "The general prompt structure which can be used among most of the LLMs is called COSTAR, each LLM might have it's own refinment for this general structure.\n",
    "### COSTAR prompt structure\n",
    "<b>Context (C)</b>\n",
    "\n",
    "Providing background information helps the LLM understand the specific scenario, ensuring relevance in its responses.\n",
    "\n",
    "Example: I am a personal productivity developer. In the realm of personal development and productivity, there is a growing demand for systems that not only help individuals set goals but also convert those goals into actionable steps. Many struggle with the transition from aspirations to concrete actions, highlighting the need for an effective goal-to-system conversion process.\n",
    "\n",
    "<b>Objective (O)</b>\n",
    "\n",
    "Clearly defining the task directs the LLM’s focus to meet that specific goals.\n",
    "\n",
    "Example: Your task is to guide me in creating a comprehensive system converter. This involves breaking down the process into distinct steps, including identifying the goal, employing the 5 Whys technique, learning core actions, setting intentions, and conducting periodic reviews. The aim is to provide a step-by-step guide for seamlessly transforming goals into actionable plans.\n",
    "\n",
    "<b>Style (S)</b>\n",
    "\n",
    "Specifying the desired writing style, such as emulating a famous personality or professional expert, guides the LLM to align its response with your needs\n",
    "\n",
    "Example: Write in an informative and instructional style, resembling a guide on personal development. Ensure clarity and coherence in the presentation of each step, catering to an audience keen on enhancing their productivity and goal attainment skills.\n",
    "\n",
    "<b>Tone (T)</b>\n",
    "\n",
    "Setting the tone ensures the response resonates with the required sentiment, whether it be formal, humorous, or empathetic.\n",
    "\n",
    "Example: Maintain a positive and motivational tone throughout, fostering a sense of empowerment and encouragement. It should feel like a friendly guide offering valuable insights.\n",
    "\n",
    "<b>Audience (A)</b>\n",
    "\n",
    "Identifying the intended audience tailors the LLM’s response to be appropriate and understandable for specific groups, such as experts or beginners.\n",
    "\n",
    "Example: The target audience is individuals interested in personal development and productivity enhancement. Assume a readership that seeks practical advice and actionable steps to turn their goals into tangible outcomes.\n",
    "\n",
    "<b>Response (R)</b>\n",
    "\n",
    "Providing the response format, like a list or JSON, ensures the LLM outputs in the required structure for downstream tasks.\n",
    "\n",
    "Example: Provide a structured list of steps for the goal-to-system conversion process. Each step should be clearly defined, and the overall format should be easy to follow for quick implementation.\n",
    "### Claude 3 prompt basics\n",
    "![alt text](/assets/c3-prompt-structure.png)  \n",
    "__Figure 2__: Claude 3 recommended prompt structure  \n",
    "\n",
    "__Use XML tags__  \n",
    "Claude is particularly familiar with prompts that have [XML tags](https://docs.anthropic.com/claude/docs/use-xml-tags) as Claude was exposed to such prompts during training. By wrapping key parts of your prompt (such as instructions, examples, or input data) in XML tags, you can help Claude better understand the context and generate more accurate outputs. This is particularly important when working with [long context window](https://docs.anthropic.com/claude/docs/long-context-window-tips) (30K+ tokens).  \n",
    "\n",
    "Using XML tags improve Claude accuracy, helps Claude understand the hierarchy and relationships within your prompt, and making it simpler to extract key information programmatically by referencing specific tag within the completion.  \n",
    "\n",
    "__Messages API__  \n",
    "The messages part of the API is an array of input messages. Claude 3 models are trained to operate on alternating user and assistant conversational turns. When creating a new Message, you specify the prior conversational turns with the messages parameter, and the model then generates the next Message in the conversation.\n",
    "\n",
    "The Messages API for Claude 3 expects input in a specific [JSON format](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html). The main object is called \"messages\", which is an array of message objects.\n",
    "\n",
    "Each message object in the \"messages\" array should have the following structure:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"role\": \"user|assistant\",\n",
    "  \"content\": \"The actual text content of the message\"\n",
    "}\n",
    "```\n",
    "\n",
    "When role is set to \"assistant\", \"content\" represents Claude response.\n",
    "\n",
    "So a simple example of the \"messages\" array with one message from a user could be:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Hello, how are you?\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "Claude 3 models are trained to operate on alternating user and assistant conversational turns.  \n",
    "You can include multiple message objects in the \"messages\" array to represent a full conversation history. Claude will then generate a response based on the entire context provided.  \n",
    "\n",
    "`user` and `assistant` messages __MUST alternate__, and messages __MUST start__ with a `user` turn. You can have multiple user & assistant pairs in a prompt (as if simulating a multi-turn conversation).\n",
    "\n",
    "Here is an example of chat:  \n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"What is the capital of France?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"The capital of France is Paris.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Recommend some places to visit in Paris.\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "__Multimodal prompts__  \n",
    "A multimodal prompt combines multiple modalities (images and text) in a single prompt. You specify the modalities in the content input field. The image should be sent to the model in base64 format. You can supply __up to 20 images__ to the model. __You can't put images in the assistant role__.  \n",
    "The following example shows how you could ask Anthropic Claude to describe the content of a supplied image:  \n",
    "```json\n",
    "{\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\", \n",
    "    \"max_tokens\": 1024,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"base64\",\n",
    "                        \"media_type\": \"image/jpeg\",\n",
    "                        \"data\": \"iVBORw...\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What's in these images?\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "While Claude’s image understanding capabilities are cutting-edge, there are some [limitations](https://docs.anthropic.com/en/docs/vision#limitations) to be aware of.  \n",
    "\n",
    "__Prefill Claude's response__  \n",
    "When using Claude, you have the ability to guide its responses and control the output format by prefilling the `assistant` content and providing clear instructions. These techniques allow you to direct Claude's actions, specify the structure and style of the generated content, and even help Claude stay in character during role-play scenarios. By leveraging prefilling and output format control, you can significantly improve Claude's performance and obtain more accurate and tailored responses.  \n",
    "\n",
    "Note that Claude will generate completions following the `assistant` content, so the prefill string is not part of the model output.  \n",
    "\n",
    "Lets see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bd140f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "    Please extract the name, size, price, and color from this product description and output it within a JSON object.\n",
    "    <description>\n",
    "        The SmartHome Mini is a compact smart home assistant available in black or white for only $49.99.\n",
    "        At just 5 inches wide, it lets you control lights, thermostats, and other connected devices via voice \n",
    "        or app—no matter where you place it in your home.\n",
    "        This affordable little hub brings convenient hands-free control to your smart devices.\n",
    "    </description>\n",
    "\"\"\"\n",
    "\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"{\"\n",
    "    }\n",
    "]\n",
    "print(get_c3_completion(messages))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd94dd9d",
   "metadata": {},
   "source": [
    "__What is a system prompts?__  \n",
    "A [system prompts](https://docs.anthropic.com/en/docs/system-prompts) is an effective way to provide context, scope, examples, guardrails, or output format to the model before presenting it with a question or task. System prompt provide an additional layer of guidance and control over Claude's output.\n",
    "System prompts try to ensure that the AI's output aligns with specific goals or tasks across various domains.  \n",
    "The system prompt should be populated with any 'global' context or rules that apply to all requests during the conversation, while anything that is unique to a particular request should be in user role.  \n",
    "While system prompts can increase Claude's robustness and resilience against unwanted behavior, they do not guarantee complete protection against jailbreaks or leaks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2056bcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"All your output must be pirate speech\"\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tell me a very short story\"\n",
    "    }\n",
    "]\n",
    "print(get_c3_completion(messages, system))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb17ece",
   "metadata": {},
   "source": [
    "\n",
    "__Anthropic Claude 3 prompt resources__  \n",
    "[Prompt Engineering with Anthropic's Claude v3 workshop](https://catalog.us-east-1.prod.workshops.aws/workshops/0644c9e9-5b82-45f2-8835-3b5aa30b1848/en-US)  \n",
    "[Anthropic's Prompt engineering docs](https://docs.anthropic.com/claude/docs/prompt-engineering)  \n",
    "[Anthropic's Prompt Engineering Interactive Tutorial - Bedrock Edition](https://github.com/aws-samples/prompt-engineering-with-anthropic-claude-v-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed6f8ef",
   "metadata": {},
   "source": [
    "\n",
    "### Mistral recommended prompt structure\n",
    "Mistral _pre-trained_ models provides a strong foundation with broad language understanding and performance across various tasks, the _instruction-tuned_ models are tailored for quick adaptation to specific tasks through fine-tuning based on instructions or prompts.\n",
    "\n",
    "The template used to build a prompt for Mistral Instruct models is defined as follows:  \n",
    "```\n",
    "<s>[INST] Instruction [/INST]\n",
    "```\n",
    "Note that `<s>` and `</s>` are special __tokens__<sup>1</sup> for beginning of sequence (__BOS__) and end of sequence (__EOS__)<sup>2</sup>, while `[INST]` and `[/INST]` are regular strings.  \n",
    "The `[INST]` and `[/INST]` 'tags' indicate to the instruction tuned model where the _INSTruction_ is.  \n",
    "\n",
    "__This prompt format must be strictly respected__. Otherwise, the model will generate sub-optimal outputs, as we are not following the template used to build a prompt for the Instruct model.\n",
    "\n",
    "__Mistral chat (multi-turn) prompt pattern__  \n",
    "For effective chatbot use-case it's recommended to use the following chat template:  \n",
    "> __\\<s\\>[INST]__ Instruction __[/INST]__ Model answer __\\</s\\>[INST]__ Follow-up instruction __[/INST]__\n",
    "\n",
    "Pay attention that after the last `Follow-up instruction [/INST]` there is no closing `</s>`. This signal the model a completion is expected for the `Follow-up instruction`.  \n",
    "\n",
    "__Note__: You might have noticed that whenever we have talked about `[INST]` and `[/INST]` we've referred to them as strings, not tokens. The simple reason for this, is that these are strings not tokens. :)  \n",
    "The model tokenizer was defined when the base model was trained. This tokenizer include two tokens, among many others, which represents `<s>` and `</s>`, but there is no token to represent `[/INST]` or `[/INST]`. So for example, when Mistral 7B was trained, the engineers created or chose a tokenizer to use, and that was that, done. When the model was instruction fine tuned, there is no need (or ability) to change the tokenizer as it's ingrained into the model already. The `[INST]` and `[/INST]` stings arrived in the instruction fine tuning dataset.\n",
    "\n",
    "[1] __Tokens__ - In order to the train a language model, the engineers first needed to convert the training text from words to numbers, because no matter what type of machine learning model we use, they all only work with numbers. For LLMs, this process is called tokenization. We train a tokenizer to convert words to numbers and output a lookup table so that once it's done, we can easily convert in both directions, words to numbers, and numbers to words. We then use this tokenizaton both during the training of the LLM, and when we are making generations with the LLM - called inference. During inference the prompt is tokenized to token ids (numbers), run through the LLM, which then generates token ids (numbers), which are de-tokenized back to words for our human squishy brains to read.  \n",
    "\n",
    "[2] __Beginning of sequence tokens__: denote the start of a sequence. If the LLM sees a bos token id, then it knows it's not missing anything that came before, and it indicates a fresh start, free from any previous context that might influence the generated output.   \n",
    "__End of sequence tokens__: denotes the end of a sequence. At infernece, If this token id is generated by the LLM, then it's an indication that the model is done, and the application that is managing the LLM can stop right there, there is nothing more to be said.\n",
    "\n",
    "Though Mistral currently doesn't support the messages API, the following code shows how you can format the prompt in similar format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0da571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "def format_instructions(instructions: List[Dict[str, str]]) -> List[str]:\n",
    "    \"\"\"Format instructions where conversation roles must alternate user/assistant/user/assistant/...\"\"\"\n",
    "    prompt: List[str] = []\n",
    "    for user, answer in zip(instructions[::2], instructions[1::2]):\n",
    "        prompt.extend([\"<s>\", \"[INST] \", (user[\"content\"]).strip(), \" [/INST] \", (answer[\"content\"]).strip(), \"</s>\"])\n",
    "    prompt.extend([\"<s>\", \"[INST] \", (instructions[-1][\"content\"]).strip(), \" [/INST] \"])\n",
    "    return \"\".join(prompt)\n",
    "\n",
    "def print_instructions(prompt: str, response: str) -> None:\n",
    "    bold, unbold = '\\033[1m', '\\033[0m'\n",
    "    print(f\"{bold}> Input{unbold}\\n{prompt}\\n\\n{bold}> Output{unbold}\\n{response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33767046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat example\n",
    "instruction1= \"Generate 4 very short sentences?\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": instruction1}\n",
    "]\n",
    "prompt = format_instructions(messages)\n",
    "response = get_mistral_completion(prompt)\n",
    "print_instructions(prompt, response)\n",
    "\n",
    "instruction2 = \"Categorize each sentence for being positive/ negative/neutral\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": instruction1},\n",
    "    {\"role\": \"assistant\", \"content\": response},\n",
    "    {\"role\": \"user\", \"content\": instruction2}\n",
    "]\n",
    "prompt = format_instructions(messages)\n",
    "response = get_mistral_completion(prompt)\n",
    "print_instructions(prompt, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773741f3",
   "metadata": {},
   "source": [
    "__Delimiters__  \n",
    "Use delimiters to specify the boundary between different sections of the text. Delimiters can be anything, for example: ```\"\"\",< >,:,###,<<< >>>```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e20506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In below example, we use ### to indicate examples and <<<>>> to indicate customer inquiry.\n",
    "inquiry = \"I forgot my card security number\"\n",
    "prompt_template = f\"\"\"\n",
    "        You are a bank customer service bot. Your task is to assess customer intent \n",
    "        and categorize customer inquiry after <<<>>> into one of the following predefined categories:\n",
    "        \n",
    "        card arrival\n",
    "        change pin\n",
    "        exchange rate\n",
    "        country support \n",
    "        cancel transfer\n",
    "        charge dispute\n",
    "        \n",
    "        If the text doesn't fit into any of the above categories, classify it as:\n",
    "        customer service\n",
    "        \n",
    "        You will only respond with the predefined category. Do not include the word \"Category\". Do not provide explanations or notes. \n",
    "        \n",
    "        ####\n",
    "        Here are some examples:\n",
    "        \n",
    "        Inquiry: How do I know if I will get my card, or if it is lost? I am concerned about the delivery process and would like to ensure that I will receive my card as expected. Could you please provide information about the tracking process for my card, or confirm if there are any indicators to identify if the card has been lost during delivery?\n",
    "        Category: card arrival\n",
    "        Inquiry: I am planning an international trip to Paris and would like to inquire about the current exchange rates for Euros as well as any associated fees for foreign transactions.\n",
    "        Category: exchange rate \n",
    "        Inquiry: What countries are getting support? I will be traveling and living abroad for an extended period of time, specifically in France and Germany, and would appreciate any information regarding compatibility and functionality in these regions.\n",
    "        Category: country support\n",
    "        Inquiry: Can I get help starting my computer? I am having difficulty starting my computer, and would appreciate your expertise in helping me troubleshoot the issue. \n",
    "        Category: customer service\n",
    "        ###\n",
    "    \n",
    "        <<<\n",
    "        Inquiry: {inquiry}\n",
    "        >>>\n",
    "        \"\"\"\n",
    "messages = [\n",
    "  { \"role\": \"user\", \"content\": prompt_template }\n",
    "]\n",
    "prompt = format_instructions(messages)\n",
    "response = get_mistral_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e80de7",
   "metadata": {},
   "source": [
    "#### Mistral AI prompt resources\n",
    "[How to Prompt Mistral AI models, and Why](https://community.aws/content/2dFNOnLVQRhyrOrMsloofnW0ckZ/how-to-prompt-mistral-ai-models-and-why)  \n",
    "[Prompting Capabilities](https://docs.mistral.ai/guides/prompting_capabilities/)  \n",
    "[A deep dive into Mistral 7B and Mixtral 8x7B, available on Amazon Bedrock](https://community.aws/content/2cZUf75V80QCs8dBAzeIANl0wzU/winds-of-change---deep-dive-into-mistral-ai-models)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6493fbf-5b8f-4248-a74d-8de2bcc91b25",
   "metadata": {},
   "source": [
    "## Inference parameters for foundation models\n",
    "Inference parameters to influence the response generated by the model. You set inference parameters in the body field of the Bedrock InvokeModel or InvokeModelWithResponseStream API. Each model may support different set of inference parameters, read [here](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html) for details specific to your model. Most of the models supports these parameters (though their naming might slightly differ). [Inference parameters](https://docs.aws.amazon.com/bedrock/latest/userguide/inference-parameters.html) describe in more depth the various common parameters. Below is simplified explenation:\n",
    "\n",
    "__max_tokens__ - The maximum  number of tokens to generate in the response. The model might stop generating tokens before reaching the value of max_tokens. Different models have different maximum values for this parameter. The number of output tokens is computed against the 'Max tokens' parameter on the Bedrock model card, which defines the maximal input + output tokens.\n",
    "\n",
    "__temperature__ - How creative is the model completion. Controls the divercity of potential next tokens. When set to zero '0' the model oputput will silightly change, if at all, between same prompt invocations. Use cases:  \n",
    "![](assets/temperature.png) />\n",
    "\n",
    "__top_k__ - Refers to the number of most likely tokens to consider at each step of generating text. It helps control the diversity of generated text by restricting the choices to the top_k most probable tokens. However, top_k doesn't enforce the probability threshold for tokens, thus you can have a token with low probability as part of your K final tokens. For this we have top_p.  \n",
    "\n",
    "__top_p__ - The percentage of most-likely tokens in a pool that the model considers for the next token. This ensures that the overall likelihood of the next token remains high.\n",
    "\n",
    "__stop_sequences__ - Custom text sequences that stop the model from generating further tokens. If the model generates a stop sequence that you specify, it will stop generating after that sequence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b8185c",
   "metadata": {},
   "source": [
    "## Basic prompting concepts\n",
    "### Zero-shot prompting\n",
    "A zero-shot prompt is a prompt that does not include any examples of the desired response, and the output is based on the model In-Context Learning.  \n",
    "Recommended uses:  \n",
    "* Comparatively long inputs and outputs, where providing examples could be cost-prohibitive or response time-prohibitive\n",
    "* Less precise output structure required, or structure can be reliably generated without providing examples.\n",
    "\n",
    "Example:\n",
    "> I thought it was pretty decent.  \n",
    "What is the sentiment of the above text? Respond with either \"Positive\" or \"Negative\"\n",
    "\n",
    "### Few-shot prompting\n",
    "Model is given multiple examples to learn from, plus the instruction. This can help the model understand exactly what a “good” response looks like. Few-shot prompting can provide additional context and guidance and improve performance. To avoid overfitting or unexpected behaviors, ensure your examples are diverse and representative of the full range of desired outputs.  \n",
    "Recommended uses:  \n",
    "* Comparatively short inputs or outputs, (try to minimize excess token consumption)\n",
    "* Precise output structure required, which could be challenging to achieve with zero-shot prompting\n",
    "\n",
    "Example:  \n",
    "> I liked it //Positive  \n",
    "It was OK //Positive  \n",
    "Couldn't believe how bad it was!//Negative  \n",
    "Not my favorite //Negative  \n",
    "This is meh.  \n",
    "What is the sentiment of the above text?\n",
    "\n",
    "### Chain of Thought Prompting (CoT)\n",
    "Technique that breaks down complex tasks through intermediate reasoning steps. Encourages model to explain its reasoning process by decomposing the solution into a series of steps. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding. Prompting for step-by-step reasoning will increase the output length, which can impact latency. Consider this tradeoff when deciding whether to use this technique. Also, there's no way to have the model think privately and only return the final answer. To make it easier to separate step-by-step output from its final response, consider using delimiters or XML tags.  \n",
    "\n",
    "Benefits:  \n",
    "* Improves performance on arithmetic, commonsense, and symbolic reasoning tasks\n",
    "* CoT allows models to decompose multi-step problems into intermediate steps\n",
    "* Provides an interpretable window into the behavior of the model. Can be used for explainability and debugging\n",
    "* Reasoning can be used for tasks such as math word problems\n",
    "* Including examples of chain of thought sequences into the exemplars of few-shot prompting can elicit reasoning ability of LLMs  \n",
    "\n",
    "Example:\n",
    "> [Rest of prompt] Before answering the question, please think about it step-by-step within \\<thinking\\>\\</thinking\\> tags. Then, provide your final answer within \\<answer\\>\\</answer\\> tags.\n",
    "\n",
    "### Prompt chaining (Prompt Decomposition)\n",
    "[Prompt chaining](https://www.promptingguide.ai/techniques/prompt_chaining) is the process of taking large prompts and breaking them down into a logical flow of smaller prompts linked together by an orchestration layer.  \n",
    "Why do we need it?  \n",
    "\n",
    "* Helps to boost the transparency of your LLM application, increases controllability, and reliability. This means that you can debug problems with model responses much more easily and analyze and improve performance in the different stages that need improvement.\n",
    "* Large prompts are hard to maintain and scale\n",
    "    * Each change could potentially impact the rest of the prompt\n",
    "    * “Dead zones” ('unreachable code' depending on input, some parts will never be used.)\n",
    "    * Longer prompts are harder for LLM’s to understand\n",
    "* Potential latency improvements by moving to smaller prompt\n",
    "* Cost optimization by moving to smaller prompt\n",
    "* Can help to useEnable switching to smaller, more performant, and more cost effective models\n",
    "\n",
    "Orchestration can be done in many ways:\n",
    "\n",
    "* Direct in code (common)\n",
    "* Traditional tools: Airflow, Step functions, etc.\n",
    "* 3rd party libraries: LangChain, Llamaindex, etc.\n",
    "\n",
    "![alt text](/assets/prompt-decomposition.png)\n",
    "\n",
    "\n",
    "<u>__Example - Prompt Chaining for Document QA__</u>:  \n",
    "One common use case of LLMs involves answering questions about a large text document. It helps if you design two different prompts where the first prompt is responsible for extracting relevant quotes to answer a question and a second prompt takes as input the quotes and original document to answer a given question. In other words, you will be creating two different prompts to perform the task of answering a question given in a document.\n",
    "\n",
    "The first prompt below extracts the relevant quotes from the document given the question.\n",
    "The second prompt then takes the relevant quotes extracted by prompt 1 and prepares a helpful response to the question given in the document and those extracted quotes.\n",
    "\n",
    "<u>Prompt 1</u>:  \n",
    "> You are a helpful assistant. Your task is to help answer a question given in a document. The first step is to extract quotes relevant to the question from the document, delimited by ####. Please output the list of quotes using \\<quotes\\>\\</quotes\\>. Respond with \"No relevant quotes found!\" if no relevant quotes were found.  \n",
    ">\n",
    "> ####{{document}}####\n",
    "\n",
    "The quotes that were returned in the first prompt can now be used as input to the second prompt below.  \n",
    "<u>Prompt 2</u>:  \n",
    "> Given a set of relevant quotes (delimited by \\<quotes\\>\\</quotes\\>) extracted from a document and the original document (delimited by ####), please compose an answer to the question. Ensure that the answer is accurate, has a friendly tone, and sounds helpful.  \n",
    ">\n",
    "> ####{{document}}####  \n",
    "> \\<quotes\\>  \n",
    "> ...  \n",
    "> \\</quotes\\>\n",
    "\n",
    "#### Tool use, Agents - are these the same?\n",
    "There seems to be a lot of confusion surrounding Tool use and Agents - whether they are the same thing. Agents and tool use (function calling) are distinct concepts. Agents employ an LLM to dynamically determine which tools to utilize and in what sequence, making them suitable for open-ended scenarios where the solution path is unknown. In contrast, tool use is suitable when you have pre-determined workflow, and you need the application to interact with other systmes (text-to-sql, code functions, calculator, etc) based on input from an LLM to perform a task, following a deterministic fashion. This approach offers better performance and control for well-defined use cases.\n",
    "\n",
    "Agents operate using the ReAct paradigm, involving multiple calls to plan, act, and evaluate, can become chatty, leading to accumulation of tokens and cost, and increased latency. Tool use, on the other hand, typically implemented by using a semantic router, which uses a lightweight model, such as Claude 3 Haiku, to classify which tool(s) to use and with what inputs. On the client side, you should extract the tool name and input from the response, and call the appropriate tool. This modular design simplifies prompt engineering, isolates validations, and enables easy maintenance and scalability.  \n",
    "By transitioning from Agents to a semantic router, latency can be significantly reduced, and prompt engineering becomes more straightforward, allowing for the execution of conditional logic based on true/false validations.\n",
    "\n",
    "### Prompt Catalog\n",
    "Curated collection of prompts designed to elicit certain behaviors, attributes, or capabilities from a generative AI system.  \n",
    "Examples:  \n",
    "* [Anthropic Claude Prompt Library](https://docs.anthropic.com/claude/prompt-library) - Collection of Claude optimized prompts for a breadth of tasks.  \n",
    "* [LangSmith Prompt Hub](https://docs.smith.langchain.com/hub/quickstart) - Discover, share, and version control prompts for LangChain and LLMs in general\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71838d45",
   "metadata": {},
   "source": [
    "## Prompt Patterns\n",
    "Prompt patterns, are similar to software design patterns, offer reusable strategies for crafting effective prompts to generate desired outputs from large language models (LLMs). As a component of prompt engineering, these patterns serve as a knowledge transfer method, providing solutions to common challenges encountered during output generation and interaction with LLMs.\n",
    "### Persona pattern\n",
    "Provide context and guidelines for the AI model when generating responses by specifying a fictional or role-based identity for the LLM to adopt when responding to prompts.  \n",
    "__When to use__:  \n",
    "When we want the LLM to adopt a specific point of view instead of generic response.  \n",
    "__Prompt pattern__:  \n",
    "_From now on, act as [persona]. Pay close attention to [details to focus on]. Provide outputs that [persona] would regarding the input._  \n",
    "__Example__:  \n",
    "> You are a history professor with expertise in ancient civilizations. You have a passion for archaeology and enjoy discussing historical mysteries. In your answer maintain a formal and knowledgeable tone suitable for academic discussions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a83775",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\n",
    "no_persona = \"\"\"\n",
    "    what is schrodinger's cat paradox?\n",
    "    Answer concisely\n",
    "\"\"\"\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": no_persona\n",
    "    }\n",
    "]\n",
    "print(f'no_persona:\\n{get_c3_completion(messages, system)}\\n')\n",
    "\n",
    "system = \"From now on answer as a quantum mechanics philosopher.\"\n",
    "print(f'with_persona:\\n{get_c3_completion(messages, system)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954ba1a6",
   "metadata": {},
   "source": [
    "### Flipped Interaction pattern\n",
    "In a typical interaction, the user asks questions to the LLM, and the LLM provides answers based on its training data and knowledge. However, in the Flipped Interaction Pattern, the LLM is encouraged to ask questions to the user to guide the process of solving a problem or achieving a specific objective.  \n",
    "__When to use__:  \n",
    "This pattern is helpful at times where it needs information you don't yet know or haven't thought of and would want to rely on LLM’s vast knowledge to help you.  \n",
    "__Prompt pattern__:  \n",
    "_From now on, I would like you to ask me questions to [do a specific task]. When you have enough information to [do the task], create [output you want]._  \n",
    "__Example__:  \n",
    "> I would like you to ask me questions to write a marketing campaign for a new product launch. When you have enough information list the steps needed for the successful launch of the product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a3c55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_flip = \"<s>[INST] what is the best AWS service to handle streaming ingestion? [/INST]\"\n",
    "print(f'no_flip:\\n{get_mistral_completion(no_flip)}\\n')\n",
    "\n",
    "with_flip = f\"\"\"\n",
    "  <s>[INST] You are an AWS solutions architect with vast knowledge in AWS analytics servies.\n",
    "  From now on, I would like you to ask me questions to suggest on a streaming architecture to a customer.\n",
    "  When you have enough information to suggest the achitecture that best fit customer needs,\n",
    "  create concise summary explaining the architecture and its benefits.  [/INST]\n",
    "\"\"\"\n",
    "print(f'with_flip:\\n{get_mistral_completion(with_flip)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa096a2b",
   "metadata": {},
   "source": [
    "\n",
    "### Question Refinement Pattern\n",
    "Ask the LLM to offer a better version of the question asked. This is especially useful when the user asking the question is not an expert in the field of question being asked and would like to rely on the knowledge an LLM has in that field.  \n",
    "__When to use__:  \n",
    "This pattern works best in areas where our initial question is broad or vague or we may not have as much information or underlying thought behind our question as LLM has from it's training data.  \n",
    "__Prompt pattern__:  \n",
    "_From now on, when I ask a question, suggest a better version of the question to use that incorporates information specific to [use case] and ask me if I would like to use your question instead._  \n",
    "__Example__:  \n",
    "> I Whenever I ask a question about implementing a feature, suggest a better version of the question that focuses on best practices and the specific programming language or framework I’m using, and ask me if I would like to use your question instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a0017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note how we place the refined question within XML tage to simplify it's extraction from the completion text\n",
    "system = \"\"\"\n",
    "    From now on, when I ask a question, suggest a better version of the question to use that \n",
    "    incorporates information specific to the question asked and ask me if I would like \n",
    "    to use your question instead. Enclose the new question within <refined> XML tags.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "    What caused the first world war?\n",
    "\"\"\"\n",
    "\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "print(get_c3_completion(messages, system))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb3c36",
   "metadata": {},
   "source": [
    "\n",
    "### Cognitive Verifier Pattern\n",
    "Breaks down complex questions into smaller, manageable sub-questions which improves it’s reasoning.  \n",
    "__When to use__:  \n",
    "This pattern is most useful when you're asking the LLM to help you complete a task that requires a logical sequence. Organizing narratives and outlining longer-form content are good candidates for this pattern. Or, when the question being asked is very high level or the user does not have much knowledge about the question.  \n",
    "__Prompt pattern__:  \n",
    "_When I ask you a question, generate three additional questions that would help you give a more accurate answer. When I have answered the three questions, combine the answers to produce the final answers to my original question._  \n",
    "__Example__:\n",
    "> I ask a question about climate change, break it down into three smaller questions that would help you provide a more accurate answer. Combine the answers to these sub-questions to give the final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccde7935",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"\n",
    "    When I ask you a question, generate three additional questions that would help you \n",
    "    give a more accurate answer. When I have answered the three questions, combine the \n",
    "    answers to produce the final answers to my original question. \n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "    Who is considered the best soccer player ever?\n",
    "\"\"\"\n",
    "\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "print(get_c3_completion(messages, system))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee071c8e",
   "metadata": {},
   "source": [
    "### Recipe Pattern\n",
    "Recipe pattern is designed to create a clear, step-by-step guide for the language model to accomplish a specific task or achieve a stated goal. In this pattern, the language model is instructed to provide a sequence of steps based on the input data provided, breaking down the process into a series of actionable steps to reach the intended outcome.  \n",
    "__When to use__:  \n",
    "When we have with limited knowledge for solving a problem and need an LLM to fill in the gaps to put together the solution.  \n",
    "__Prompt pattern__:  \n",
    "_I want you to act as [use case]. I want you to provide me list of  suggestions[use case]. Your suggestion should be [specific, actionable …]. Do not provide [use case]._  \n",
    "__Example__:\n",
    "> I need to deploy a cloud application. Some of the steps requires logging in to the cloud console, instantiating an instance, and installing dependencies. Please provide a complete sequence of steps. Please fill in any missing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d257ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_recipe = \"\"\"\n",
    "    <s>[INST]How can I calculate the net profit of my company?[/INST]\n",
    "\"\"\"\n",
    "print(f'no_recipe:\\n{get_mistral_completion(no_recipe)}')\n",
    "\n",
    "with_recipe = \"\"\"\n",
    "    <s>[INST] I am trying to calculate the net profit of my company. \n",
    "    I know that I need to take into account the total revenues, \n",
    "    cost of sales and taxation in USA. Provide a complete sequence of steps. \n",
    "    Fill in any missing steps. Identify any unnecessary steps.[/INST]\n",
    "\"\"\"\n",
    "print(f'\\nwith_recipe:\\n{get_mistral_completion(with_recipe)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7d2d33",
   "metadata": {},
   "source": [
    "\n",
    "### Template Pattern\n",
    "Ask for a completion in a specific format.  \n",
    "__When to use__:  \n",
    "Use this when you want LLM to generate a response that aligns with a defined structure.  \n",
    "__Prompt pattern__:  \n",
    "_Please generate a response following the given template : [Template]_  \n",
    "__Example__:  \n",
    "> Please generate a response following the given template: Introduction — [Introductory sentence]. Main Points — [Key points to be covered]. Conclusion — [Concluding statement]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd1f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example include the variables within the prompt itself to make a point.\n",
    "# In production use-case you will probably fetch this data from a database.\n",
    "prompt = \"\"\"\n",
    "    <s>[INST]I am going to provide a template for your output. Everything in all caps is a placeholder. \n",
    "    Any time that you generate text, try to fit it into one of the placeholders that I list. \n",
    "    Please preserve the formatting and overall template that I provide.\n",
    "    Write an email for Marco's 38th birthday and gift 38 points: \n",
    "    Dear PERSON,\n",
    "    \n",
    "    Greating from AnyHotel! \n",
    "    As a gift for your birthday we are happy to recognize NUMBER_OF_POINTS Points for your loyality.\n",
    "    \n",
    "    Best wishis,\n",
    "    AnyHotel Staff[/INST]\n",
    "\"\"\"\n",
    "print(get_mistral_completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1a07e1",
   "metadata": {},
   "source": [
    "### Reflection Pattern\n",
    "The Reflection Pattern is most effective when you're asking for something that will be challenging for you to judge.  \n",
    "__When to use__:  \n",
    "Understanding the rationale behind the output proves highly beneficial when a user desires to evaluate the legitimacy of the LLM's response and comprehend the process by which the LLM formulated a specific answer. Moreover, this approach enables users to refine and optimize their prompts, as they gain deeper insights into the LLM's method of generating outputs.  \n",
    "__Prompt pattern__:  \n",
    "_When you provide an answer, please explain the reasoning and assumptions behind your response. If possible, use specific examples or evidence to support your answer of why [prompt topic] is the best. Moreover, please address any potential ambiguities or limitations in your answer, in order to provide a more complete and accurate response._  \n",
    "__Example__:  \n",
    "> Tell me ideas for a trendy tiktok video;  When you provide an answer, please explain the reasoning and assumptions behind your response. If possible, use specific examples or evidence to support your answer of why this is a great idea for Tiktok video is the best. Moreover, please address any potential ambiguities or limitations in your answer, in order to provide a more complete and accurate response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dcb52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\n",
    "prompt = \"\"\"\n",
    "    Is Back the darkest color?\n",
    "\"\"\"\n",
    "\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "print(f'no_reflection:\\n{get_c3_completion(messages, system)}')\n",
    "\n",
    "system = \"\"\"\n",
    "    When you provide an answer, please explain the reasoning and assumptions behind your response. \n",
    "    If possible, use specific examples or evidence to support your answer. \n",
    "    Moreover, please address any potential ambiguities or limitations in your answer, \n",
    "    in order to provide a more complete and accurate response.\n",
    "\"\"\"\n",
    "\n",
    "print(f'\\nwith_reflection:\\n{get_c3_completion(messages, system)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce7e22e",
   "metadata": {},
   "source": [
    "### Ask for Input Pattern\n",
    "The Ask for Input Pattern is about seeking specific content from end user for further processing or action.  \n",
    "__When to use__:  \n",
    "Use this when you want the LLM to ask follow-up questions to complete a task.  \n",
    "__Example__:  \n",
    "> From now on, I am going to cut/paste email chains into our conversation. You will summarize what each person’s points are in the email chain. You will provide your summary as a series of sequential bullet points. At the end, list any open questions or action items directly addressed to me. My name is Jill Smith. Ask me for the first email chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eddf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will be asked in input text for the example to complete\n",
    "system = \"\"\n",
    "prompt = \"\"\"\n",
    "    From now on i will describe the location, lighting and watering availability, \n",
    "    and level of desired caring required and you will provide mw with a suggestion on which plant to buy. \n",
    "    Now ask me the for the first location\n",
    "\"\"\"\n",
    "\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "response = get_c3_completion(messages, system)\n",
    "print(response)\n",
    "location = input(\"Enter your location: \")\n",
    "\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": response\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": location\n",
    "    }\n",
    "]\n",
    "print(get_c3_completion(messages, system))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361682f3",
   "metadata": {},
   "source": [
    "### Fact Checklist Pattern\n",
    "The Fact Check List Pattern involves articulating the crucial facts that reside within the output, serving as a foundation for understanding and verifying the information. If these facts are incorrect, they could drastically affect the overall validity or veracity of the information.  \n",
    "__When to use__:  \n",
    "This method is handy for verifying response validity. It generates a fact list from the response to validate it, rather than analyzing the entire response.  \n",
    "__Prompt pattern__:  \n",
    "After you generate a [Task] summary, compile a list of the key facts. Insert this fact list at the [position] of the summary. Include the main points that would affect the overall understanding of the [Task].  \n",
    "__Example__:  \n",
    "> After you generate a news article summary, compile a list of the key facts. Insert this fact list at the end of the summary. Include the main points that would affect the overall understanding of the news story.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09d3966",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\n",
    "prompt = \"\"\"\n",
    "    What was the share of CO2 emissions by wildfires in 2020 globally?\n",
    "\"\"\"\n",
    "\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "print(f'no_fact:\\n{get_c3_completion(messages, system)}')\n",
    "\n",
    "system = \"\"\"\n",
    "    From now on, when you generate an answer, create a set of facts that the answer depends on that \n",
    "    should be fact-checked and list this set of facts at the end of your output. \n",
    "\"\"\"\n",
    "\n",
    "print(f'\\nwith_fact:\\n{get_c3_completion(messages, system)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd9a372",
   "metadata": {},
   "source": [
    "## Promt challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa80e2d",
   "metadata": {},
   "source": [
    "### Challenge #1\n",
    "Given this prompt 'what is the capital of France?' modify it so it will output only single word: 'Paris.'  \n",
    "Model to use: Haiku  \n",
    "Try first before expanding the hint.  \n",
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "    Ask Claude to skip the preamble\n",
    "    <details>\n",
    "        <summary>Prompt</summary>\n",
    "        What is the capital of France? Skip the preamble\n",
    "    </details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e942ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the prompt and run\n",
    "prompt = \"what is the capital of France?\"\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "print(get_c3_completion(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d11d33",
   "metadata": {},
   "source": [
    "\n",
    "### Challenge #2\n",
    "Write a prompt to solve this word puzzle: Using only addition, add eight 8s to get the number 1,000  \n",
    "Model to use: Mistral 7B-Instruct  \n",
    "Try first before expanding the hint.  \n",
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "    Be specific, and direct the model attention to the appropriate direction: Word puzzle; Use the Persona pattern.\n",
    "    <details>\n",
    "        <summary>Prompt</summary>\n",
    "        You are an expert in solving word puzzles. Using only addition, add eight 8s to get the number 1,000\n",
    "    </details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026a421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with your own prompt and run\n",
    "prompt = \"\"\n",
    "messages = [\n",
    "    { \"role\": \"user\", \"content\": prompt }\n",
    "]\n",
    "prompt = format_instructions(messages)\n",
    "response = get_mistral_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645cc2cc",
   "metadata": {},
   "source": [
    "### Challenge #3\n",
    "Write a prompt to solve this question correctly: Bill has as many sisters as he has brothers. Are there more boys or girls in the family?  \n",
    "Model to use: Mistral 7B-Instruct  \n",
    "Try first before expanding the hint.  \n",
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "    Try Chain of Thoughts\n",
    "    <details>\n",
    "        <summary>Prompt</summary>\n",
    "        Bill has as many sisters as he has brothers. Are there more boys or girls in the family? Think carfully step by step\n",
    "    </details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2633a1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with your own prompt and run\n",
    "prompt = \"\"\n",
    "messages = [\n",
    "    { \"role\": \"user\", \"content\": prompt }\n",
    "]\n",
    "prompt = format_instructions(messages)\n",
    "response = get_mistral_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8396feb7",
   "metadata": {},
   "source": [
    "### Challenge #4\n",
    "Write a prompt that instructs the model to engage in a conversational interaction with you to determine the most suitable AWS service for capturing and processing large volumes of stream events in near real-time.  \n",
    "Model to use: Claude 3 Haiku  \n",
    "Try first before expanding the hint.  \n",
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "    Flipped Interaction pattern\n",
    "    <details>\n",
    "        <summary>Prompt</summary>\n",
    "        I need your help in selecting the most appropriate AWS service for capturing and processing large volumes of stream events in near real-time. Please engage in a conversational interaction with me, asking questions to gather the necessary information about the use case and my requirements. Once you have enough details, recommend the optimal AWS service(s) and explain your reasoning.\n",
    "    </details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb246ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with your own prompt and run\n",
    "prompt = \"\"\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "print(get_c3_completion(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7837ae78",
   "metadata": {},
   "source": [
    "### Challenge #5\n",
    "Modify the prompt to correctly answer: Is 'x' in the equation below solved correctly?  \n",
    "2x - 3 = 9  \n",
    "2x = 6  \n",
    "x = 3  \n",
    "Model to use: Claude 3 Haiku  \n",
    "Try first before expanding the hint.  \n",
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "    Ask Claude to review the solution carefully  \n",
    "    <details>\n",
    "        <summary>Prompt</summary>\n",
    "        <p>Review the equation and the solution steps carefully. Is 'x' in the equation below solved correctly?</p>\n",
    "        <p>2x - 3 = 9</p>\n",
    "        <p>2x = 6</p>\n",
    "        <p>x = 3</p>\n",
    "    </details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e8eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the prompt and run\n",
    "prompt = \"\"\"\"\n",
    "  Is 'x' in the equation below solved correctly?\n",
    "  2x - 3 = 9\n",
    "  2x = 6\n",
    "  x = 3 \n",
    "\"\"\"\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "print(get_c3_completion(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5e2c3f",
   "metadata": {},
   "source": [
    "### Challenge #6\n",
    "Write a prompt that calculate the result of: 1984135 * 9343116  \n",
    "Model to use: Claude 3 Haiku  \n",
    "Try first before expanding the hint.  \n",
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "    As of now, LLMs have limitations in performing complex numerical computations accurately.\n",
    "    LLMs are trained on vast amounts of text data, which allows them to understand and generate human-like text, but they are not specifically designed for performing complex mathematical operations with high precision. While LLMs can perform basic arithmetic operations, their performance tends to degrade as the complexity of the mathematical operations increases, especially when dealing with large numbers or operations involving many steps.\n",
    "    <details>\n",
    "        <summary>Solution</summary>\n",
    "        Use tools instead.\n",
    "    </details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ba16fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with your own prompt and run\n",
    "prompt = \"\"\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "print(get_c3_completion(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e746bc",
   "metadata": {},
   "source": [
    "### Challenge #7\n",
    "Write a tool-use prompt that assesses the question and decides which math function(s) to call, and with what arguments. Try your solution on: Please solve 1984135 * 9343116  \n",
    "Model to use: Claude 3 Haiku  \n",
    "Try first before expanding the hint.  \n",
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "    Review this section of the Claude workshop <a href='https://catalog.us-east-1.prod.workshops.aws/workshops/0644c9e9-5b82-45f2-8835-3b5aa30b1848/en-US/lessons/lab-10-2-tool-use'>here</a>.\n",
    "    <details>\n",
    "        <summary>Solution</summary>\n",
    "        Open <a href='challenge-7-solution.py'>challenge-7-solution.py</a> to review the complete solution.\n",
    "    </details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7184b073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to multiply two numbers\n",
    "def mul(a, b):\n",
    "    return a * b\n",
    "# Function to sum two numbers\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "# Function to subtract two numbers\n",
    "def sub(a, b):\n",
    "    return a - b\n",
    "\n",
    "system = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "stop_sequence = [\"Human: \"]\n",
    "\n",
    "prompt = \"\"\"\"\n",
    "    Please solve 1984135 * 9343116\n",
    "\"\"\"\n",
    "\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "print(get_c3_completion(messages, system = system, stop_sequence = stop_sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07087567",
   "metadata": {},
   "source": [
    "### Challenge #8\n",
    "Write a prompt that read all words within image images/twelve-word-challenge.png <img src=\"images/twelve-word-challenge.png\" alt=\"image\" style=\"width:300px;height:auto;\">  \n",
    "Model to use: Claude 3 Haiku  \n",
    "Try first before expanding the hint.  \n",
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "    TBD\n",
    "    <details>\n",
    "        <summary>Prompt</summary>\n",
    "        Review the image carefully. Notice that some words may be in skew angles. Extract all words within the image and count them. Output a full numbered list of all words regardless where they appear in the image\n",
    "    </details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5159609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with your own prompt and run\n",
    "# Function that accept image file location and return base64 representation of the image\n",
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "def get_base64_image(image_path):\n",
    "    \"\"\"\n",
    "    Converts an image file to its base64 representation.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): The file path of the image.\n",
    "        \n",
    "    Returns:\n",
    "        str: The base64 representation of the image.\n",
    "    \"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        encoded_string = base64.b64encode(image_file.read())\n",
    "    return encoded_string.decode('utf-8')\n",
    "\n",
    "prompt = \"\"\n",
    "messages=[\n",
    "    {\n",
    "      \"type\": \"image\",\n",
    "      \"source\": {\n",
    "        \"type\": \"base64\",\n",
    "        \"media_type\": \"image/jpeg\",\n",
    "        \"data\": get_base64_image(\"images/twelve-word-challenge.png\")\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "print(get_c3_completion(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8702e48",
   "metadata": {},
   "source": [
    "### Challenge #9\n",
    "The prompt `I am trying to market this product, help me think of an advertisement script on social media` results a refusal from Haiku. Revise the prompt, and make Haiku draft social media post marketing the product shown in images/mite-and-insect.png  \n",
    "![Product]\"[/assets/mite-and-insect.png]  \n",
    "\n",
    "Model to use: Claude 3 Haiku  \n",
    "Try first before expanding the hint.  \n",
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "    Use user role, and add more specific context for the task.\n",
    "    <details>\n",
    "        <summary>Solution Step 1</summary>\n",
    "        system = You are a marketing assistant named Joe, working for AnyCompany. The company manufactures and sells products across industries including materials, agriculture, and manufacturing. Your goal is to author a social media posts to market the company products. Please respond to the user’s question within &lt;response&gt;&lt;/response&gt; tags.\n",
    "    </details>\n",
    "    <details>\n",
    "        <summary>Solution Step 2</summary>\n",
    "        Add prefilling:\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"[Joe from AnyCompany] &lt;response&gt;\"\n",
    "        }\n",
    "    </details>\n",
    "    <details>\n",
    "        <summary>Note</summary>\n",
    "        Try to put the system prompts text in the prompt instead. Evaluate the model response difference. Which is better?\n",
    "        One effective way to help Claude stay in character is by using system prompts. System prompts help set the tone, establish the character’s personality, and provide guidelines for the model to follow. By prefilling the response with [Joe from AnyCompany], you’re forcing Claude to acknowledge that it’s role-playing as that persona and to generate responses that logically follow what the persona would say. \n",
    "    </details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f946b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\n",
    "prompt = \"I am trying to market this product, help me think of an advertisement script on social media\"\n",
    "messages=[\n",
    "    {\n",
    "      \"type\": \"image\",\n",
    "      \"source\": {\n",
    "        \"type\": \"base64\",\n",
    "        \"media_type\": \"image/jpeg\",\n",
    "        \"data\": get_base64_image(\"images/mite-and-insect.png\")\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "print(get_c3_completion(messages, system))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9d8348",
   "metadata": {},
   "source": [
    "### Challenge #9\n",
    "Invoke this prompt and verify response validity:\n",
    "Is this statement correct: If two charged objects are placed in an isolated system and one object loses 5 Coulombs of charge, the other object gain exactly 5 Coulombs of charge  \n",
    "Model to use: Claude 3 Haiku  \n",
    "Try first before expanding the hint.  \n",
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "    Use Fact Checklist Pattern.\n",
    "    <details>\n",
    "        <summary>Solution</summary>\n",
    "        system = After you generate your answer, compile list of of key facts on which you base your answer.\n",
    "    </details>\n",
    "    <details>\n",
    "        <summary>Consideration point</summary>\n",
    "        Try the solution prompt with Mistral 7B Instruct and compare the results. Mistral response answer wrongly. Even setting appropriate role doesn't fix it's response. It seems Mistral wasn't trained on relevant physics data. For such use-cases, Mistral is probably not the model of choice.\n",
    "    </details>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f03af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with your own prompt and run\n",
    "system = \"\"\n",
    "prompt = \"If two charged objects are placed in an isolated system and one object loses 5 Coulombs of charge, the other object gain exactly 5 Coulombs of charge\"\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "print(get_c3_completion(messages, system))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
